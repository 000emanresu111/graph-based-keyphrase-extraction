{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/boudinfl/pke.git\n",
    "!python -m nltk.downloader stopwords\n",
    "!python -m nltk.downloader universal_tagset\n",
    "!python -m spacy download en\n",
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pke\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "LANGUAGE_CODES = {\n",
    "    'it': 'italian',\n",
    "    'en': 'english',\n",
    "    'de': 'german',\n",
    "    'es': 'spanish',\n",
    "    'fr': 'french',\n",
    "    'nl': 'dutch'\n",
    "}\n",
    "\n",
    "PART_OF_SPEECH = {'NOUN', 'ADJ', 'ADV', 'PROPN'}\n",
    "\n",
    "STOPLIST = list(string.punctuation) + ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph-based algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextRankSequences(text, language, n, window, top_percent, pos):\n",
    "    extractor = pke.unsupervised.TextRank()\n",
    "\n",
    "    extractor.load_document(input=text, language=language,\n",
    "                            normalization='lemmatization')\n",
    "    extractor.candidate_selection(pos=pos)\n",
    "    extractor.candidate_weighting(\n",
    "        window=window, pos=pos, top_percent=top_percent)\n",
    "\n",
    "    return extractor.get_n_best(n, stemming=False)\n",
    "\n",
    "\n",
    "def getTopicRankSequences(text, language, n, window, threshold, pos):\n",
    "    extractor = pke.unsupervised.TopicRank()\n",
    "\n",
    "    extractor.load_document(input=text, language=language,\n",
    "                            normalization='lemmatization')\n",
    "\n",
    "    stoplist = STOPLIST + stopwords.words(LANGUAGE_CODES[language])\n",
    "\n",
    "    extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "    extractor.candidate_weighting(threshold=threshold, method='average')\n",
    "\n",
    "    return extractor.get_n_best(n, stemming=False)\n",
    "\n",
    "\n",
    "def getMultipartiteRankSequences(text, language, n, alpha, threshold, pos):\n",
    "    extractor = pke.unsupervised.MultipartiteRank()\n",
    "\n",
    "    extractor.load_document(input=text, language=language,\n",
    "                            normalization='lemmatization')\n",
    "\n",
    "    stoplist = STOPLIST + stopwords.words(LANGUAGE_CODES[language])\n",
    "\n",
    "    extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "    extractor.candidate_weighting(alpha=alpha,\n",
    "                              threshold=threshold,\n",
    "                              method='average')\n",
    "    \n",
    "    return extractor.get_n_best(n, stemming=False)\n",
    "\n",
    "\n",
    "def getPositionRankSequences(text, language, n, window, maximum_word_number):\n",
    "    pos = {'NOUN', 'PROPN', 'ADJ', 'ADV'}\n",
    "    grammar = \"NP: {<ADJ>*<NOUN|PROPN|ADV>+}\"\n",
    "    \n",
    "    extractor = pke.unsupervised.PositionRank()\n",
    "    extractor.load_document(input=text,\n",
    "                        language=language,\n",
    "                        normalization=None)\n",
    "    \n",
    "    extractor.candidate_selection(grammar=grammar,\n",
    "                              maximum_word_number=maximum_word_number)\n",
    "    \n",
    "    extractor.candidate_weighting(window=window,\n",
    "                              pos=pos)\n",
    "    \n",
    "    return extractor.get_n_best(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get extracted sentences based on keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printExtractedSentences(results):\n",
    "    keyphrases_list = []\n",
    "    longest_lenght = max([len(record[0]) for record in results])\n",
    "\n",
    "    for record in results:\n",
    "        keyphrase = record[0]\n",
    "        keyphrases_list.append(keyphrase)\n",
    "        score = str(record[1])\n",
    "        print(' '.join([keyphrase.ljust(longest_lenght), score]))\n",
    "\n",
    "    extracted_sentences = []\n",
    "\n",
    "    for keyphrase in keyphrases_list:\n",
    "        for i in range(0, len(sentences_list)):\n",
    "            if keyphrase.strip().lower() in sentences_list[i].strip().lower():\n",
    "                extracted_sentences.append(\" \".join(sentences_list[i].lower().split()))\n",
    "\n",
    "    unique_sentences = set(extracted_sentences)\n",
    "\n",
    "    return keyphrases_list, unique_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print found keyphrases in bold within the entire text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import re\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "def boldify_text(text, keyphrases_list):\n",
    "    \n",
    "    regex = re.compile(r'\\b(?:%s)\\b' % '|'.join(keyphrases_list), re.I)\n",
    "    i = 0; output = \"\"\n",
    "    for m in regex.finditer(text):\n",
    "        output += \"\".join([text[i:m.start()],\n",
    "                        \"***\",\n",
    "                        text[m.start():m.end()],\n",
    "                        \"***\"])\n",
    "        i = m.end()\n",
    "    printmd(\"\".join([output, text[m.end():]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I've had this product since 2015. The trimmer works really good, but unfortunately the battery life is terrible. After about a year of use, I have to expedite my shaves before the trimmer completely dies and needs to be recharged. Expensive product for a terrible battery life.\n",
    "\"\"\"\n",
    "language = 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From text to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*** Sentences ***\\n\")\n",
    "sentences_list = nltk.tokenize.sent_tokenize(text)\n",
    "for i in range(0, len(sentences_list)):\n",
    "    print(str(i) + \") \" + \" \".join(sentences_list[i].split()) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "results = getTextRankSequences(text=text, language=language, n=10, window=2, top_percent=0.33, pos=PART_OF_SPEECH)\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time)) \n",
    "keyphrases_list, unique_sentences = printExtractedSentences(results)\n",
    "boldify_text(text, keyphrases_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TopicRank results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "results = getTopicRankSequences(text=text, language=language, n=20, window=2, threshold=0.74, pos=PART_OF_SPEECH)\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time)) \n",
    "keyphrases_list, unique_sentences = printExtractedSentences(results)\n",
    "boldify_text(text, keyphrases_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultipartiteRank results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "results = getMultipartiteRankSequences(text, language, n=20, alpha=1.1, threshold=0.74, pos=PART_OF_SPEECH)\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time)) \n",
    "keyphrases_list, unique_sentences = printExtractedSentences(results)\n",
    "boldify_text(text, keyphrases_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PositionRank results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "results = getPositionRankSequences(text, language, n=20, window=10, maximum_word_number=3)\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time)) \n",
    "keyphrases_list, unique_sentences = printExtractedSentences(results)\n",
    "boldify_text(text, keyphrases_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
